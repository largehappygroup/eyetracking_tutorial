{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zachkaras/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import easyocr\n",
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "reader = easyocr.Reader(['en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "# directory that contains screenshots for all the stimuli\n",
    "pics = os.listdir('./final_stimuli')\n",
    "print(len(pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "# This file contains all the information for the stimuli (fuction name, id #, summaries, code)\n",
    "stimdf = pd.read_csv('../stimuli/pruned_seeds2.csv')\n",
    "print(len(stimdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "49\n",
      "46\n",
      "31\n",
      "37\n",
      "57\n",
      "77\n",
      "55\n",
      "43\n",
      "50\n",
      "35\n",
      "58\n",
      "53\n",
      "39\n",
      "58\n",
      "29\n",
      "74\n",
      "50\n",
      "52\n",
      "50\n",
      "43\n",
      "35\n",
      "62\n",
      "49\n",
      "58\n",
      "34\n",
      "31\n",
      "31\n",
      "42\n",
      "55\n",
      "33\n",
      "57\n",
      "37\n",
      "49\n",
      "61\n",
      "45\n",
      "62\n",
      "53\n",
      "44\n",
      "41\n",
      "100\n",
      "37\n",
      "29\n",
      "55\n",
      "49\n",
      "30\n",
      "31\n",
      "41\n",
      "65\n",
      "39\n",
      "33\n",
      "53\n",
      "82\n",
      "31\n",
      "57\n",
      "37\n",
      "42\n",
      "48\n",
      "40\n",
      "53\n",
      "45\n",
      "43\n",
      "55\n",
      "25\n",
      "47\n",
      "59\n",
      "41\n",
      "71\n",
      "64\n",
      "77\n",
      "52\n",
      "63\n",
      "26\n",
      "37\n",
      "56\n",
      "63\n",
      "29\n",
      "37\n",
      "36\n",
      "51\n",
      "78\n",
      "79\n",
      "51\n",
      "47\n",
      "36\n",
      "57\n",
      "53\n",
      "32\n",
      "51\n",
      "52\n",
      "40\n",
      "38\n",
      "36\n",
      "37\n",
      "50\n",
      "72\n",
      "60\n",
      "53\n",
      "47\n",
      "46\n",
      "73\n",
      "52\n",
      "42\n",
      "47\n",
      "54\n",
      "66\n",
      "54\n",
      "39\n",
      "37\n",
      "47\n",
      "79\n",
      "48\n",
      "35\n",
      "43\n",
      "64\n",
      "59\n",
      "59\n",
      "59\n",
      "57\n",
      "47\n",
      "48\n",
      "61\n",
      "38\n",
      "37\n",
      "43\n",
      "35\n",
      "46\n",
      "40\n",
      "46\n",
      "87\n",
      "35\n",
      "39\n",
      "70\n",
      "43\n",
      "21\n",
      "39\n",
      "35\n",
      "88\n",
      "40\n",
      "42\n",
      "42\n",
      "40\n",
      "47\n",
      "65\n",
      "42\n",
      "58\n",
      "53\n",
      "36\n",
      "48\n",
      "33\n",
      "54\n",
      "57\n",
      "48\n",
      "46\n",
      "36\n",
      "57\n",
      "53\n",
      "47\n",
      "55\n",
      "54\n",
      "61\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "## read through pngs and find corresponding function\n",
    "for pic in pics:\n",
    "    name = re.split('.png', pic)[0]\n",
    "    \n",
    "    # temp contains all the images for each word, split by function name\n",
    "    os.mkdir('./temp/{name}'.format(name=name))\n",
    "    boxfile = '{name}_boxes.csv'.format(name=name)\n",
    "    df = pd.DataFrame()\n",
    "    row = np.where(stimdf['name'] == name) # finding row specific to each function\n",
    "    i = row[0][0]\n",
    "    func = stimdf['function'][i] # getting actual Java code for each stimulus\n",
    "    split = re.split(\" |\\n\", func) # splitting each function by spaces and newlines to get word level \n",
    "    split = list(filter(None, split))\n",
    "    \n",
    "    # now with all the code split up, create actual bounding boxes on images\n",
    "    # basically just a pipeline with cv2\n",
    "    \n",
    "    img = cv2.imread('./final_stimuli/{pic}'.format(pic=pic)) # \n",
    "    img = img[100:1000, 10:1150]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (8, 8))\n",
    "    dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "    contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    print(len(contours)) # number of words caught by bounding boxes\n",
    "    \n",
    "    unpredicted = 0 # need to do these manually\n",
    "    for ii, box in enumerate(contours):\n",
    "        box = contours[(len(contours)-1)-ii]\n",
    "        x, y, w, h = cv2.boundingRect(box)  # coordinates, width, and height\n",
    "        tangle = cv2.rectangle(img, (x, y), (x+w, y+h),(0, 255, 0, 2))  # drawing the rectangle\n",
    "        word_img = img[y+1:(y+1)+(h-1), x+1:(x+1)+(w-1)] # actual pixel values for word\n",
    "        resized = cv2.resize(word_img, (w*5, h*5),interpolation=cv2.INTER_CUBIC) # bumping up size to improve OCR\n",
    "\n",
    "        # Convert image to grayscale\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply thresholding to remove noise\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # Perform dilation to make characters more prominent\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
    "        # We generate a copy of the image and apply a dilation kernel \n",
    "        # and median blur on it. - ChatGPT\n",
    "        blurred = cv2.medianBlur(dilated, 1)\n",
    "        \n",
    "        # Perform erosion to remove any remaining noise\n",
    "        erosion = cv2.erode(blurred, kernel, iterations=1)\n",
    "        inverted = 255 - erosion\n",
    "\n",
    "        # Perform OCR on the inverted image\n",
    "        result = reader.readtext(inverted)\n",
    "        \n",
    "        if not result: # sometimes word was null\n",
    "            #cv2.imwrite(\"./temp/{func}/{c}_unknown.png\".format(func=name, c=unpredicted), inverted)\n",
    "            temp = pd.DataFrame([['unknown_{i}'.format(i=unpredicted), x, y, w, h]]) # still adding the row\n",
    "            df = pd.concat([df, temp], ignore_index=True)\n",
    "            unpredicted += 1\n",
    "        for r in result:\n",
    "            #cv2.imwrite(\"./temp/{func}/{c}.png\".format(c=r[1],func=name), inverted)\n",
    "            temp = pd.DataFrame([[re.sub(\" \", \"\", r[1]), x, y, w, h]])\n",
    "            df = pd.concat([df, temp], ignore_index=True)\n",
    "\n",
    "    #cv2.imwrite(\"./temp/{name}/{c}_func.png\".format(c=name, name=name), img)\n",
    "    df.columns = ['word', 'x', 'y', 'width', 'height']\n",
    "    \n",
    "    df = df.sort_values(['y','x']) # some characters had different heights (B vs. +), so the below code \n",
    "    count = 0                      # standardizes row values and sorts each row\n",
    "    standard = df.iloc[0, 2]       # now we have bounding box files that read sequentially in order\n",
    "    for i, row in df.iterrows():\n",
    "        if i < len(df)-1:\n",
    "            diff = df.iloc[i+1, 2]-row[2]\n",
    "            if diff > 20: # heuristic for new row height\n",
    "                standard = df.iloc[i+1, 2]\n",
    "            else:\n",
    "                row[2] = standard\n",
    "                df.iloc[i+1, 2] = standard\n",
    "    df = df.sort_values(['y', 'x'])\n",
    "    #pd.DataFrame.to_csv(df, \"./word_coordinates/{name}_boxes.csv\".format(name=name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
